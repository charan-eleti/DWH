package com.edifice
import java.text.SimpleDateFormat
import java.util.Date

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{StringType, StructField, StructType}

import scala.collection.mutable.ArrayBuffer

case class Edifice(retailer:String, account: String, year_day: String, UPC: String, STORENUMBER: String,QS: String,QA: String,QR: String,QU: String,XR: String,flag: String)


object edificeReport{

  val schema = StructType(Array(
    StructField("retailer", StringType),
    StructField("account", StringType),
    StructField("year_day", StringType),
    StructField("UPC", StringType),
    StructField("STORENUMBER", StringType),
    StructField("QS", StringType),
    StructField("QA", StringType),
    StructField("QR", StringType),
    StructField("QU", StringType),
    StructField("XR", StringType),
    StructField("flag", StringType)
  ))

  def reportDate(s: String): String ={
    var  simpleDateFormat:SimpleDateFormat = new SimpleDateFormat("mm/dd/yyyy")
    var  date:Date = simpleDateFormat.parse(s)
    val year_day = new SimpleDateFormat("yyyymmdd").format(date)
    return year_day
  }

  def mapSections(reportText: Array[String]): Map[String, String] = {
    var reportName: String = ""
    var retailer: String = ""
    var account: String = ""
    var year_day: String = ""
    var flag: String = ""
    var reportLine: String = ""
    var reportBuffer: ArrayBuffer[String] = new ArrayBuffer
    var reportMap: Map[String, String] = Map()
    //var reportMap: Map[String, String] = Map()
    for (line <- reportText){
        val header = line.split("\r\n").head
        val trail = line.split("\r\n").tail
        val splitHeader = header.split('|')
        retailer = splitHeader(1).replace(" ", "_")
        account = splitHeader(2).replace(" - ", "_").replace(" ", "_")
        year_day = reportDate(splitHeader(3))
        flag = splitHeader(4)
      //println(line)
        //val reportLine = line.split("\n").map(x => retailer + '|' + account + '|' + x + '|' + flag)
      //val reportLine = line.replace("\n", "###").split("###").map(x => retailer + '|' + account + '|' + x + '|' + flag)
      //println(line)
      val accountLine = line.split("\r\n").filter(line => line != header).filter(line => line != trail).map(x => retailer + '|' + account + '|' + year_day + '|' + x + '|' + flag)
      //println(reportLine(1))
      //val rowLine = accountLine.map(_.split("|")).map(x => Edifice(x(0),x(1).trim,x(2),x(3).trim,x(4).trim,x(5).trim,x(6).trim,x(7).trim,x(8).trim,x(9).trim,x(10).trim))
      reportName = retailer + "_" + account + "_" + year_day
      for (line <- accountLine){
        reportLine += line + "\r\n"
      }

      reportMap += (reportName -> reportLine)
      reportLine = ""
    }
    return reportMap
  }

  def splitSectionAndClean(text: String): Array[String] = {
    val splitLines = text.replace("\r\nHDR", "####HDR").split("####")
    return splitLines
  }
/*
  def parseData(txt: String): Row ={
    val testRegex = """\s*("""
    val matchData = List(textRegex,)
    val matchData(retailer,account,UPC,STORENUMBER,QS,QA,QR,QU,XR,flag) = txt

  }
*/
  def parse(keyValue: (String, String)) = {
    val reportMap = mapSections(splitSectionAndClean(keyValue._2))
    /*
    for ((a,b) <- reportMap){
      val r = b.split("\r\n").map(_.split("|")).toList
    }
    */
    reportMap
  }
}
object processEdifice {
  def main(args: Array[String]) {

    val sparkSession = SparkSession.builder.
      master("local")
      .appName("edifice splitFiles")
      .getOrCreate()
    val sc = sparkSession.sparkContext

    val dataRecRDD = sc.wholeTextFiles("""C:\Intellij\EDW\src\main\resources\YETICOOLERS_ACADEMYSPORTS_ACADEMYSPORTS_20180624.txt""")
    //println(dataRecRDD.collect.toList)
    val data_rec = dataRecRDD.flatMap(x => edificeReport.parse(x))
    //val data_rec_RDD = data_rec.map(_._2).map(_.split("\r\n")).map(x => Edifice(x(0),x(1).trim,x(2),x(3).trim,x(4).trim,x(5).trim,x(6).trim,x(7).trim,x(8).trim,x(9).trim,x(10).trim))
    val data_rec_RDD = data_rec.map(_._2)
    //val dataRecDF = data_rec_RDD.toDF
    //for ((k,v) <- data_rec) printf("key: %s, value: %s\n", k, v)
    //println(rdd.collect.toList

    val dataRecDF = sparkSession.createDataFrame(,edificeReport.schema)
    dataRecDF.printSchema()
    dataRecDF.show()

  }
}
